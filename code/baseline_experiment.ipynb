{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import STL10\n",
        "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile, errno\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "try:\n",
        "    from imageio import imsave\n",
        "except:\n",
        "    from scipy.misc import imsave"
      ],
      "metadata": {
        "id": "ObQyFBuqMjCS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load STL-10 Dataset"
      ],
      "metadata": {
        "id": "qOOxfBGMVaF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image shape\n",
        "HEIGHT = 96\n",
        "WIDTH = 96\n",
        "DEPTH = 3\n",
        "\n",
        "# size of a single image in bytes\n",
        "SIZE = HEIGHT * WIDTH * DEPTH\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = './data'\n",
        "\n",
        "# url of the binary data\n",
        "DATA_URL = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
        "\n",
        "# path to the binary train file with image data\n",
        "DATA_PATH = './data/stl10_binary/train_X.bin'\n",
        "\n",
        "# path to the binary train file with labels\n",
        "LABEL_PATH = './data/stl10_binary/train_y.bin'"
      ],
      "metadata": {
        "id": "KROrRLkdVzYb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract():\n",
        "    \"\"\"\n",
        "    Download and extract the STL-10 dataset\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
        "        print('Downloaded', filename)\n",
        "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
      ],
      "metadata": {
        "id": "doVMDFYBVb64"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIfZDmR-YTej",
        "outputId": "1ad95078-fdf7-4d87-cf27-08b7d9223573"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stl10_binary.tar.gz 99.99%"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "FgMLJZlbMrZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "fzkbW0SOMnx6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STL10Pair(STL10):\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.labels[index]\n",
        "        img = Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            pos_1 = self.transform(img)\n",
        "            pos_2 = self.transform(img)\n",
        "\n",
        "        return pos_1, pos_2, target"
      ],
      "metadata": {
        "id": "ZgMice9kMuuc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "LhEY9QqxMw9K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(kernel_size=int(0.1 * 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"
      ],
      "metadata": {
        "id": "K2nN6GmfM0IB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "FaxONCMOM2Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Model(nn.Module):\n",
        "#     def __init__(self, feature_dim=128):\n",
        "#         super(Model, self).__init__()\n",
        "\n",
        "#         self.f = []\n",
        "#         for name, module in resnet50().named_children():\n",
        "#             if name == 'conv1':\n",
        "#                 module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "#             if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
        "#                 self.f.append(module)\n",
        "#         # encoder\n",
        "#         self.f = nn.Sequential(*self.f)\n",
        "#         # projection head\n",
        "#         self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),\n",
        "#                                nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.f(x)\n",
        "#         feature = torch.flatten(x, start_dim=1)\n",
        "#         out = self.g(feature)\n",
        "#         return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)"
      ],
      "metadata": {
        "id": "D6b1UsefNMvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, feature_dim=128, arch=\"resnet18\"):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.f = []\n",
        "\n",
        "        if arch == \"resnet18\":\n",
        "            module = resnet18()\n",
        "            in_size = 512\n",
        "        elif arch == \"resnet34\":\n",
        "            module = resnet34()\n",
        "            in_size = 512\n",
        "        elif arch == \"resnet50\":\n",
        "            module = resnet50()\n",
        "            in_size = 2048\n",
        "        else:\n",
        "            raise Exception(\"Unknown module {}\".format(repr(arch)))\n",
        "        for name, module in module.named_children():\n",
        "            if name == \"conv1\":\n",
        "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
        "                self.f.append(module)\n",
        "        # encoder\n",
        "        self.f = nn.Sequential(*self.f)\n",
        "        # projection head\n",
        "        self.g = nn.Sequential(\n",
        "            nn.Linear(in_size, 512, bias=False),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, feature_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.g(feature)\n",
        "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)"
      ],
      "metadata": {
        "id": "hI07_Y24gfXd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment"
      ],
      "metadata": {
        "id": "V4HGCzJoM3Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_mask(batch_size):\n",
        "    negative_mask = torch.ones((batch_size, 2 * batch_size), dtype=bool)\n",
        "    for i in range(batch_size):\n",
        "        negative_mask[i, i] = 0\n",
        "        negative_mask[i, i + batch_size] = 0\n",
        "\n",
        "    negative_mask = torch.cat((negative_mask, negative_mask), 0)\n",
        "    return negative_mask"
      ],
      "metadata": {
        "id": "MCQhVgsVM85f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_loader, train_optimizer, temperature, debiased, tau_plus):\n",
        "    net.train()\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
        "    for pos_1, pos_2, target in train_bar:\n",
        "        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n",
        "        feature_1, out_1 = net(pos_1)\n",
        "        feature_2, out_2 = net(pos_2)\n",
        "\n",
        "        # neg score\n",
        "        out = torch.cat([out_1, out_2], dim=0)\n",
        "        neg = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
        "        mask = get_negative_mask(batch_size).cuda()\n",
        "        neg = neg.masked_select(mask).view(2 * batch_size, -1)\n",
        "\n",
        "        # pos score\n",
        "        pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
        "        pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "        # estimator g()\n",
        "        if debiased:\n",
        "            N = batch_size * 2 - 2\n",
        "            Ng = (-tau_plus * N * pos + neg.sum(dim = -1)) / (1 - tau_plus)\n",
        "            # constrain (optional)\n",
        "            Ng = torch.clamp(Ng, min = N * np.e**(-1 / temperature))\n",
        "        else:\n",
        "            Ng = neg.sum(dim=-1)\n",
        "\n",
        "        # contrastive loss\n",
        "        loss = (- torch.log(pos / (pos + Ng) )).mean()\n",
        "\n",
        "        train_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_optimizer.step()\n",
        "\n",
        "        total_num += batch_size\n",
        "        total_loss += loss.item() * batch_size\n",
        "\n",
        "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n",
        "\n",
        "    return total_loss / total_num"
      ],
      "metadata": {
        "id": "pO7v4belM_I-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l-kfJrqsMdBx"
      },
      "outputs": [],
      "source": [
        "# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n",
        "def test(net, memory_data_loader, test_data_loader):\n",
        "    net.eval()\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
        "            feature, out = net(data.cuda(non_blocking=True))\n",
        "            feature_bank.append(feature)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(memory_data_loader.dataset.labels, device=feature_bank.device)\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, _, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature, out = net(data)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "            sim_matrix = torch.mm(feature, feature_bank)\n",
        "            # [B, K]\n",
        "            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n",
        "            # [B, K]\n",
        "            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n",
        "            sim_weight = (sim_weight / temperature).exp()\n",
        "\n",
        "            # counts for each class\n",
        "            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n",
        "            # [B*K, C]\n",
        "            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1).long(), value=1.0)\n",
        "            # weighted score ---> [B, C]\n",
        "            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            test_bar.set_description('KNN Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n",
        "                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n",
        "\n",
        "    return total_top1 / total_num * 100, total_top5 / total_num * 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
        "    parser.add_argument('--feature_dim', default=128, type=int, help='Feature dim for latent vector')\n",
        "    parser.add_argument('--temperature', default=0.5, type=float, help='Temperature used in softmax')\n",
        "    parser.add_argument('--tau_plus', default=0.1, type=float, help='Positive class priorx')\n",
        "    parser.add_argument('--k', default=200, type=int, help='Top k most similar images used to predict the label')\n",
        "    parser.add_argument('--batch_size', default=256, type=int, help='Number of images in each mini-batch')\n",
        "    parser.add_argument('--epochs', default=500, type=int, help='Number of sweeps over the dataset to train')\n",
        "    parser.add_argument('--debiased', default=True, type=bool, help='Debiased contrastive loss or standard loss')\n",
        "\n",
        "    # # args parse\n",
        "    # args = parser.parse_args()\n",
        "    # feature_dim, temperature, tau_plus, k = args.feature_dim, args.temperature, args.tau_plus, args.k\n",
        "    # batch_size, epochs, debiased = args.batch_size, args.epochs,  args.debiased\n",
        "    feature_dim = 128\n",
        "    temperature = 0.5\n",
        "    tau_plus = 0.1\n",
        "    k = 200\n",
        "    batch_size = 256\n",
        "    epochs = 50\n",
        "    debiased = True\n",
        "\n",
        "    # data prepare\n",
        "    train_data = STL10Pair(root='data', split='train+unlabeled', transform=train_transform)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
        "                              drop_last=True)\n",
        "    memory_data = STL10Pair(root='data', split='train', transform=test_transform)\n",
        "    memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_data = STL10Pair(root='data', split='test', transform=test_transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # model setup and optimizer config\n",
        "    model = Model(feature_dim).cuda()\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "    c = len(memory_data.classes)\n",
        "    print('# Classes: {}'.format(c))\n",
        "\n",
        "    train_losses = []\n",
        "    test_accs_1 = []\n",
        "    test_accs_5 = []\n",
        "\n",
        "    # training loop\n",
        "    if not os.path.exists('results'):\n",
        "        os.mkdir('results')\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, train_loader, optimizer, temperature, debiased, tau_plus)\n",
        "        train_losses.append(train_loss)\n",
        "        test_acc_1, test_acc_5 = test(model, memory_loader, test_loader)\n",
        "        test_accs_1.append(test_accs_1)\n",
        "        test_accs_5.append(test_acc_5)\n",
        "        torch.save(model.state_dict(), 'results/model_{}.pth'.format(epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idYF5_bZNGNU",
        "outputId": "2bd58316-f4b0-4e98-980e-1d3861afc86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Classes: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: [1/50] Loss: 4.9135: 100%|██████████| 410/410 [04:54<00:00,  1.39it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.64it/s]\n",
            "KNN Test Epoch: [1/50] Acc@1:44.47% Acc@5:93.47%: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]\n",
            "Train Epoch: [2/50] Loss: 4.6359: 100%|██████████| 410/410 [04:45<00:00,  1.44it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "KNN Test Epoch: [2/50] Acc@1:52.41% Acc@5:96.54%: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]\n",
            "Train Epoch: [3/50] Loss: 4.5245: 100%|██████████| 410/410 [04:47<00:00,  1.43it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.67it/s]\n",
            "KNN Test Epoch: [3/50] Acc@1:54.30% Acc@5:96.74%: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]\n",
            "Train Epoch: [4/50] Loss: 4.4648: 100%|██████████| 410/410 [04:45<00:00,  1.44it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
            "KNN Test Epoch: [4/50] Acc@1:56.17% Acc@5:97.12%: 100%|██████████| 32/32 [00:17<00:00,  1.86it/s]\n",
            "Train Epoch: [5/50] Loss: 4.4193: 100%|██████████| 410/410 [04:52<00:00,  1.40it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.54it/s]\n",
            "KNN Test Epoch: [5/50] Acc@1:58.79% Acc@5:97.51%: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s]\n",
            "Train Epoch: [6/50] Loss: 4.3916: 100%|██████████| 410/410 [04:51<00:00,  1.41it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "KNN Test Epoch: [6/50] Acc@1:59.76% Acc@5:97.62%: 100%|██████████| 32/32 [00:16<00:00,  1.88it/s]\n",
            "Train Epoch: [7/50] Loss: 4.3677: 100%|██████████| 410/410 [04:48<00:00,  1.42it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
            "KNN Test Epoch: [7/50] Acc@1:59.94% Acc@5:97.72%: 100%|██████████| 32/32 [00:17<00:00,  1.82it/s]\n",
            "Train Epoch: [8/50] Loss: 4.3456: 100%|██████████| 410/410 [04:47<00:00,  1.43it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.75it/s]\n",
            "KNN Test Epoch: [8/50] Acc@1:61.12% Acc@5:97.59%: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]\n",
            "Train Epoch: [9/50] Loss: 4.3269: 100%|██████████| 410/410 [04:46<00:00,  1.43it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.82it/s]\n",
            "KNN Test Epoch: [9/50] Acc@1:61.84% Acc@5:97.72%: 100%|██████████| 32/32 [00:17<00:00,  1.83it/s]\n",
            "Train Epoch: [10/50] Loss: 4.3118: 100%|██████████| 410/410 [04:43<00:00,  1.44it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.81it/s]\n",
            "KNN Test Epoch: [10/50] Acc@1:62.22% Acc@5:97.89%: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]\n",
            "Train Epoch: [11/50] Loss: 4.2980: 100%|██████████| 410/410 [04:42<00:00,  1.45it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.71it/s]\n",
            "KNN Test Epoch: [11/50] Acc@1:62.66% Acc@5:97.95%: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]\n",
            "Train Epoch: [12/50] Loss: 4.2858: 100%|██████████| 410/410 [04:43<00:00,  1.44it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "KNN Test Epoch: [12/50] Acc@1:64.08% Acc@5:98.04%: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]\n",
            "Train Epoch: [13/50] Loss: 4.2726: 100%|██████████| 410/410 [04:43<00:00,  1.45it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.77it/s]\n",
            "KNN Test Epoch: [13/50] Acc@1:64.28% Acc@5:98.16%: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]\n",
            "Train Epoch: [14/50] Loss: 4.2604: 100%|██████████| 410/410 [04:43<00:00,  1.45it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.54it/s]\n",
            "KNN Test Epoch: [14/50] Acc@1:63.90% Acc@5:98.11%: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]\n",
            "Train Epoch: [15/50] Loss: 4.2523: 100%|██████████| 410/410 [04:45<00:00,  1.44it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.72it/s]\n",
            "KNN Test Epoch: [15/50] Acc@1:64.34% Acc@5:98.29%: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]\n",
            "Train Epoch: [16/50] Loss: 4.2444: 100%|██████████| 410/410 [04:49<00:00,  1.42it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.83it/s]\n",
            "KNN Test Epoch: [16/50] Acc@1:64.64% Acc@5:98.15%: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]\n",
            "Train Epoch: [17/50] Loss: 4.2307: 100%|██████████| 410/410 [04:51<00:00,  1.41it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.74it/s]\n",
            "KNN Test Epoch: [17/50] Acc@1:65.61% Acc@5:98.19%: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]\n",
            "Train Epoch: [18/50] Loss: 4.2244: 100%|██████████| 410/410 [04:49<00:00,  1.41it/s]\n",
            "Feature extracting: 100%|██████████| 20/20 [00:07<00:00,  2.52it/s]\n",
            "KNN Test Epoch: [18/50] Acc@1:65.29% Acc@5:98.21%: 100%|██████████| 32/32 [00:17<00:00,  1.83it/s]\n",
            "Train Epoch: [19/50] Loss: 4.2466:   4%|▍         | 16/410 [00:13<03:55,  1.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear evaluation"
      ],
      "metadata": {
        "id": "mkkntMjuQevt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_class, pretrained_path):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        model = Model().cuda()\n",
        "        model = nn.DataParallel(model)\n",
        "        model.load_state_dict(torch.load(pretrained_path))\n",
        "\n",
        "        self.f = model.module.f\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out"
      ],
      "metadata": {
        "id": "7pc4fwxnbbZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train or test for one epoch\n",
        "def train_val(net, data_loader, train_optimizer):\n",
        "    is_train = train_optimizer is not None\n",
        "    net.train() if is_train else net.eval()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)\n",
        "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            out = net(data)\n",
        "            loss = loss_criterion(out, target)\n",
        "\n",
        "            if is_train:\n",
        "                train_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                train_optimizer.step()\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "\n",
        "            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
        "                                     .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,\n",
        "                                             total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))\n",
        "\n",
        "    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100"
      ],
      "metadata": {
        "id": "HGpKoZgqbeBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Linear Evaluation')\n",
        "    parser.add_argument('--model_path', type=str, default='results/model_400.pth',\n",
        "                        help='The pretrained model path')\n",
        "    parser.add_argument('--batch_size', type=int, default=512, help='Number of images in each mini-batch')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of sweeps over the dataset to train')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    model_path, batch_size, epochs = args.model_path, args.batch_size, args.epochs\n",
        "    train_data = STL10(root='data', split='train', transform=train_transform)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_data = STL10(root='data', split='test', transform=test_transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = Net(num_class=len(train_data.classes), pretrained_path=model_path).cuda()\n",
        "    for param in model.f.parameters():\n",
        "        param.requires_grad = False\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "    optimizer = optim.Adam(model.module.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "    results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "               'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer)\n",
        "        test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None)"
      ],
      "metadata": {
        "id": "4TNMMZiCOXEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}